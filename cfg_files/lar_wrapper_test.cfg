[global]
group = dune
experiment = dune
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
basename = protoduneana_\\\${SAM_PROJECT}_\\\${consumerid}
#numjobs = 10

[executable]
name = ./lar_wrapper.py 
arg_1 = -c
arg_2 = \\\${CONDOR_DIR_INPUT}/grid_pduneana_Prod4_job_g4rw_systs_new.fcl
arg_3 = -j
arg_4 = %(basename)s_temp.json
arg_5 = --rootname
arg_6 = %(basename)s.root
arg_7 = --sam-application-family
arg_8 = protoduneana
arg_9 = -n
arg_10 = 50


[stage]
lines_1 '+FERMIHTC_AutoRelease=True'
lines_2 '+FERMIHTC_GraceMemory=1024'
lines_3 '+FERMIHTC_GraceLifetime=3600'

[env_pass]
IFDH_DEBUG=1
IFDH_CP_MAXRETRIES=3
OUTPUT_DIR = /pnfs/dune/scratch/users/calcuttj/pduneana_test/mc/
EXTRA_DIR = .

[sam_consumer]
limit = 8

[submit]
G  = %(group)s
OS = SL7
memory = 3000MB
expected-lifetime = 4h
n_files_per_job = 5
dataset = calcuttj_PDSPProd4a_MC_1GeV_reco1_sce_datadriven_v1_first_10 
resource-provides = usage_model=OFFSITE,OPPORTUNISTIC,DEDICATED
f_0 = /pnfs/dune/resilient/users/calcuttj/job/grid_pduneana_Prod4_job_g4rw_systs_new.fcl
f_1 = /pnfs/dune/resilient/users/calcuttj/lar_wrapper.py 
f_2 = /pnfs/dune/resilient/users/calcuttj/mergeMeta.py
c = "has_avx==True"
tar_file_name = /pnfs/dune/resilient/users/calcuttj/pduneana.tar


[job_output]
addoutput = *root
dest = \\\${OUTPUT_DIR}/\\\${EXTRA_DIR}/\\\${CLUSTER}_\\\${PROCESS}

[job_output_1]
addoutput = *json
dest = \\\${OUTPUT_DIR}/\\\${EXTRA_DIR}/\\\${CLUSTER}_\\\${PROCESS}


[job_setup]
source_1 = /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh
setup_local = True
ifdh_art = True
userscript = True
prescript = cp ${CONDOR_DIR_INPUT}/lar_wrapper.py ./
prescript_1 = cp ${CONDOR_DIR_INPUT}/mergeMeta.py ./
prescript_2 = chmod +x lar_wrapper.py
