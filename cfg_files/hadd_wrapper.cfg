[global]
group = dune
experiment = dune
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
basename = protoduneana_\\\${SAM_PROJECT}_\\\${consumerid}
listname = root_list.txt
listloc = /pnfs/dune/resilient/users/calcuttj/%(listname)s

[executable]
name = ./hadd_wrapper.py 
arg_1 = -r
arg_2 = \\\${CONDOR_DIR_INPUT}/%(listname)s
arg_3 = --usedb
arg_4 = 1
arg_5 = -o
arg_6 = hadd_wrapper_test.root
arg_7 = -N
arg_8 = 10

[stage]
lines_1 '+FERMIHTC_AutoRelease=True'
lines_2 '+FERMIHTC_GraceMemory=1024'
lines_3 '+FERMIHTC_GraceLifetime=3600'

[env_pass]
IFDH_DEBUG=1
IFDH_CP_MAXRETRIES=3
OUTPUT_DIR = /pnfs/dune/scratch/users/calcuttj/pduneana_test/mc/
EXTRA_DIR = .


[submit]
G  = %(group)s
OS = SL7
memory = 3000MB
disk = 30GB
expected-lifetime = 12h
resource-provides = usage_model=OFFSITE,OPPORTUNISTIC,DEDICATED
f_0 = /pnfs/dune/resilient/users/calcuttj/hadd_wrapper.py 
f_1 = /pnfs/dune/resilient/users/calcuttj/mergeMeta.py 
f_2 = %(listloc)s
c = "has_avx==True"
tar_file_name = /pnfs/dune/resilient/users/calcuttj/pduneana.tar



[job_output]
addoutput = *root
dest = \\\${OUTPUT_DIR}/\\\${EXTRA_DIR}/\\\${CLUSTER}_\\\${PROCESS}

[job_output_1]
addoutput = *json
dest = \\\${OUTPUT_DIR}/\\\${EXTRA_DIR}/\\\${CLUSTER}_\\\${PROCESS}

[job_setup]
source_1 = /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh
setup_local = True
prescript = cp ${CONDOR_DIR_INPUT}/hadd_wrapper.py ./
prescript_1 = cp ${CONDOR_DIR_INPUT}/mergeMeta.py ./
prescript_2 = chmod +x hadd_wrapper.py
